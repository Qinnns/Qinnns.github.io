---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My name is Haoqin Sun, and I am currently a third-year Ph.D. student at the College of Computer Science, Nankai University, working under the supervision of Professor Yong Qin. My research interests include:
- Speech Emotion Recognition 
- Multimodal
- Audio Understanding

# üìù Publications 
<!-- for example -->
<!-- - <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ACL 2024</span><span style="color:red">(Oral)</span> [GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators](https://aclanthology.org/2024.acl-long.5.pdf), **<u>Yuchen Hu</u>**, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng. [[Code]](https://github.com/YUCHEN005/GenTranslate) [[Data]](https://huggingface.co/datasets/PeacefulData/HypoTranslate) -->

- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">Arxiv</span> [EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations](https://arxiv.org/pdf/2505.23018), **Haoqin Sun**, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin.
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">INTERSPEECH 2025</span> [RA-CLAP: Relation-Augmented Emotional Speaking Style Contrastive Language-Audio Pretraining For Speech Retrieval](https://arxiv.org/pdf/2505.19437), **Haoqin Sun**, Jingguang Tian, Jiaming Zhou, Hui Wang, Jiabei He, Shiwan Zhao, Xiangyu Kong, Desheng Hu, Xinkang Xu, Xinhui Hu, Yong Qin.
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2025</span> [Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework](https://ieeexplore.ieee.org/abstract/document/10889485), **Haoqin Sun**, Shiwan Zhao, Shaokai Li, Xiangyu Kong, Xuechen Wang, Aobo Kong, Jiaming Zhou, Yong Chen, Wenjia Zeng, Yong Qin.
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">INTERSPEECH 2024</span> [Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition](https://www.isca-archive.org/interspeech_2024/sun24e_interspeech.pdf), **Haoqin Sun**, Shiwan Zhao, Xiangyu Kong, Xuechen Wang, Hui Wang, Jiaming Zhou, Yong Qin
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">ICASSP 2024</span> [Fine-grained disentangled representation learning for multimodal emotion recognition](https://ieeexplore.ieee.org/abstract/document/10447667), **Haoqin Sun**, Shiwan Zhao, Xuechen Wang, Wenjia Zeng, Yong Chen, Yong Qin.
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">TASLP</span> [A Discriminative Feature Representation Method Based on Cascaded Attention Network With Adversarial Strategy for Speech Emotion Recognition](https://ieeexplore.ieee.org/abstract/document/10045019), Yang Liu\*, **Haoqin Sun\***, Wenbo Guan, Yuqi Xia, Yongwei Li, Masashi Unoki, Zhen Zhao. (**student first author**)
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">INTERSPEECH 2023</span> [Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions](https://www.isca-archive.org/interspeech_2023/liu23b_interspeech.pdf), Yang Liu\*, **Haoqin Sun\***, Geng Chen, Qingyue Wang, Zhen Zhao, Xugang Lu, Longbiao Wang. (**student first author**)
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">SPEECH COM</span> [Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework](https://www.sciencedirect.com/science/article/pii/S0167639322000309), Yang Liu\*, **Haoqin Sun\***, Wenbo Guan, Yuqi Xia, Zhen Zhao. (**student first author**)
- <span style="display:inline-block; background-color:#00369F; color:#fff; padding:0px 7px; margin-right:5px; font-size:13px;">INTERSPEECH 2022</span> [Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition](https://www.isca-archive.org/interspeech_2022/liu22aa_interspeech.pdf), Yang Liu\*, **Haoqin Sun\***, Wenbo Guan, Yuqi Xia, Zhen Zhao. (**student first author**)



# üìñ Educations
*September 2023 - June 2027 (expected)*, Nankai University
- Ph.D. in Computer Science. Supervisor: Prof. Yong Qin

*September 2020 - June 2023*, Qingdao University of Science and Technology
- M.S. in Software Engineering. Supervisor: Prof. Yang Liu

*September 2016 - June 2020*, Qingdao University
- B.S. in Digital Media Technology.


# üíª Internships
- Research Intern, [Alibaba](https://www.alibaba.com), China (*May 2025 - now*)
- Research Intern, [Hithink Royalflush](https://www.tonghuashuncn.com/), China (*July 2024 - March 2025*)

# üßë‚Äçüî¨ Services
**Reviewer:** &nbsp; 
- IEEE Transactions on Audio, Speech and Language Processing, Speech Communication, Expert Systems With Applications
- ICASSP (24/25), INTERSPEECH(23/24/25)

# üéñ Honors and Awards
- China National Scholarship, Ministry of Education of China, *2023*
- Outstanding Graduate Award of Qingdao, *2023*


Thanks for the template of <a href="https://rayeren.github.io">Yi Ren</a>
